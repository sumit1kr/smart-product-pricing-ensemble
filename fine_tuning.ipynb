{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ SAFE BERT-ENHANCED PIPELINE - RAM OPTIMIZED\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# GPU check\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úÖ Device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1Ô∏è‚É£ MEMORY-OPTIMIZED DATA LOADING\n",
        "# ============================================================================\n",
        "print(\"\\nüì• Loading data with memory optimization...\")\n",
        "\n",
        "def optimize_dtypes(df):\n",
        "    \"\"\"Aggressive memory optimization\"\"\"\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'float64':\n",
        "            df[col] = df[col].astype('float32')\n",
        "        elif df[col].dtype == 'int64':\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "# Load only necessary columns\n",
        "train = pd.read_csv('/content/drive/MyDrive/train_cleaned.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/test_cleaned.csv')\n",
        "\n",
        "# Save test IDs before optimization\n",
        "test_ids = test['sample_id'].values.copy()\n",
        "\n",
        "# Optimize memory\n",
        "train = optimize_dtypes(train)\n",
        "test = optimize_dtypes(test)\n",
        "\n",
        "print(f\"‚úÖ Train: {train.shape}, Test: {test.shape}\")\n",
        "print(f\"‚úÖ Memory: Train={train.memory_usage(deep=True).sum() / 1024**2:.1f}MB\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2Ô∏è‚É£ SAFE BERT EMBEDDINGS (MEMORY CONTROLLED)\n",
        "# ============================================================================\n",
        "print(\"\\nüß† Extracting BERT embeddings (safe mode)...\")\n",
        "\n",
        "text_col = 'catalog_content_clean'\n",
        "\n",
        "class SafeBERTEmbedder:\n",
        "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = device\n",
        "\n",
        "    def load(self):\n",
        "        print(f\"   Loading {self.model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def embed(self, texts, batch_size=16, max_len=64):\n",
        "        \"\"\"Safe embedding with smart pooling\"\"\"\n",
        "        if self.tokenizer is None:\n",
        "            self.load()\n",
        "\n",
        "        embeddings = []\n",
        "        total = len(texts)\n",
        "\n",
        "        for i in range(0, total, batch_size):\n",
        "            batch = texts[i:i+batch_size].tolist()\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                batch,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_len,\n",
        "                return_tensors='pt'\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Generate embeddings with BETTER pooling strategy\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "                # Use CLS token (pooler_output) if available, else mean pooling\n",
        "                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "                    emb = outputs.pooler_output.cpu().numpy()\n",
        "                else:\n",
        "                    # Attention-mask weighted mean pooling (better than simple mean)\n",
        "                    attention_mask = inputs['attention_mask']\n",
        "                    token_embeddings = outputs.last_hidden_state\n",
        "\n",
        "                    # Expand attention mask for broadcasting\n",
        "                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "\n",
        "                    # Weighted mean\n",
        "                    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "                    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                    emb = (sum_embeddings / sum_mask).cpu().numpy()\n",
        "\n",
        "                embeddings.append(emb.astype(np.float32))\n",
        "\n",
        "            # Cleanup\n",
        "            del inputs, outputs\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            if (i // batch_size) % 20 == 0:\n",
        "                print(f\"      {min(i+batch_size, total)}/{total} processed\")\n",
        "\n",
        "            # Extra RAM safety: cleanup every 50 batches\n",
        "            if (i // batch_size) % 50 == 0:\n",
        "                gc.collect()\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "# Generate BERT embeddings\n",
        "if text_col in train.columns:\n",
        "    train_texts = train[text_col].fillna('').astype(str)\n",
        "    test_texts = test[text_col].fillna('').astype(str)\n",
        "\n",
        "    embedder = SafeBERTEmbedder()\n",
        "\n",
        "    print(\"   Training set embeddings...\")\n",
        "    bert_train = embedder.embed(train_texts, batch_size=16)\n",
        "\n",
        "    # RAM safety: process and reduce immediately\n",
        "    print(\"   Reducing train dimensions...\")\n",
        "    pca = PCA(n_components=48, random_state=42)  # Increased from 32\n",
        "    bert_train_reduced = pca.fit_transform(bert_train).astype(np.float32)\n",
        "\n",
        "    # Free train embeddings before test\n",
        "    del bert_train\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"   Test set embeddings...\")\n",
        "    bert_test = embedder.embed(test_texts, batch_size=16)\n",
        "\n",
        "    print(\"   Reducing test dimensions...\")\n",
        "    bert_test_reduced = pca.transform(bert_test).astype(np.float32)\n",
        "\n",
        "    print(f\"‚úÖ BERT: {bert_train_reduced.shape[1]} dims (var: {pca.explained_variance_ratio_.sum():.2%})\")\n",
        "\n",
        "    # Cleanup\n",
        "    del bert_test, embedder\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "else:\n",
        "    bert_train_reduced = np.zeros((len(train), 0), dtype=np.float32)\n",
        "    bert_test_reduced = np.zeros((len(test), 0), dtype=np.float32)\n",
        "\n",
        "# ============================================================================\n",
        "# 3Ô∏è‚É£ ENHANCED TF-IDF FEATURES\n",
        "# ============================================================================\n",
        "print(\"\\nüìù Extracting TF-IDF features...\")\n",
        "\n",
        "if text_col in train.columns:\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=100,  # Increased from 50\n",
        "        stop_words='english',\n",
        "        min_df=3,\n",
        "        max_df=0.9,\n",
        "        ngram_range=(1, 2),\n",
        "        dtype=np.float32\n",
        "    )\n",
        "\n",
        "    tfidf_train = tfidf.fit_transform(train_texts)\n",
        "    tfidf_test = tfidf.transform(test_texts)\n",
        "\n",
        "    # Reduce dimensions (keep more variance)\n",
        "    svd = TruncatedSVD(n_components=25, random_state=42)  # Increased from 15\n",
        "    tfidf_train_reduced = svd.fit_transform(tfidf_train).astype(np.float32)\n",
        "    tfidf_test_reduced = svd.transform(tfidf_test).astype(np.float32)\n",
        "\n",
        "    print(f\"‚úÖ TF-IDF: {tfidf_train_reduced.shape[1]} dims (var: {svd.explained_variance_ratio_.sum():.2%})\")\n",
        "\n",
        "    del tfidf_train, tfidf_test\n",
        "    gc.collect()\n",
        "else:\n",
        "    tfidf_train_reduced = np.zeros((len(train), 0), dtype=np.float32)\n",
        "    tfidf_test_reduced = np.zeros((len(test), 0), dtype=np.float32)\n",
        "\n",
        "# ============================================================================\n",
        "# 4Ô∏è‚É£ BASIC NUMERIC FEATURES (SAFE)\n",
        "# ============================================================================\n",
        "print(\"\\nüî¢ Processing basic features...\")\n",
        "\n",
        "basic_cols = ['item_pack_qty', 'catalog_len']\n",
        "available_cols = [c for c in basic_cols if c in train.columns]\n",
        "\n",
        "if available_cols:\n",
        "    train_basic = train[available_cols].fillna(0).astype(np.float32).values\n",
        "    test_basic = test[available_cols].fillna(0).astype(np.float32).values\n",
        "\n",
        "    # Safe transformations\n",
        "    train_basic_log = np.log1p(np.abs(train_basic))\n",
        "    test_basic_log = np.log1p(np.abs(test_basic))\n",
        "\n",
        "    train_basic_sqrt = np.sqrt(np.abs(train_basic))\n",
        "    test_basic_sqrt = np.sqrt(np.abs(test_basic))\n",
        "\n",
        "    # Combine\n",
        "    train_basic_all = np.hstack([train_basic, train_basic_log, train_basic_sqrt])\n",
        "    test_basic_all = np.hstack([test_basic, test_basic_log, test_basic_sqrt])\n",
        "\n",
        "    print(f\"‚úÖ Basic: {train_basic_all.shape[1]} dims\")\n",
        "else:\n",
        "    train_basic_all = np.zeros((len(train), 0), dtype=np.float32)\n",
        "    test_basic_all = np.zeros((len(test), 0), dtype=np.float32)\n",
        "\n",
        "# ============================================================================\n",
        "# 5Ô∏è‚É£ ENHANCED IMAGE FEATURES (EXPLICIT NaN HANDLING)\n",
        "# ============================================================================\n",
        "print(\"\\nüñºÔ∏è Extracting image features...\")\n",
        "\n",
        "def extract_image_features(df):\n",
        "    \"\"\"Image URL features with explicit NaN handling\"\"\"\n",
        "    features = []\n",
        "    for url in df['image_link']:\n",
        "        # Explicit NaN handling\n",
        "        if pd.isna(url) or url == '' or str(url).lower() == 'nan':\n",
        "            # Default features for missing URLs\n",
        "            feat = [0] * 8\n",
        "        else:\n",
        "            url_str = str(url).lower()\n",
        "            feat = [\n",
        "                1 if url_str.startswith('https') else 0,\n",
        "                1 if 'amazon' in url_str else 0,\n",
        "                1 if 'cloudfront' in url_str else 0,\n",
        "                1 if '.jpg' in url_str else 0,\n",
        "                1 if '.png' in url_str else 0,\n",
        "                len(url_str),\n",
        "                url_str.count('/'),\n",
        "                url_str.count('-'),\n",
        "            ]\n",
        "        features.append(feat)\n",
        "    return np.array(features, dtype=np.float32)\n",
        "\n",
        "train_image = extract_image_features(train)\n",
        "test_image = extract_image_features(test)\n",
        "\n",
        "print(f\"‚úÖ Image: {train_image.shape[1]} dims\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6Ô∏è‚É£ COMBINE ALL FEATURES\n",
        "# ============================================================================\n",
        "print(\"\\nüß© Combining features...\")\n",
        "\n",
        "X_train_raw = np.hstack([\n",
        "    train_basic_all,\n",
        "    bert_train_reduced,\n",
        "    tfidf_train_reduced,\n",
        "    train_image\n",
        "])\n",
        "\n",
        "X_test_raw = np.hstack([\n",
        "    test_basic_all,\n",
        "    bert_test_reduced,\n",
        "    tfidf_test_reduced,\n",
        "    test_image\n",
        "])\n",
        "\n",
        "print(f\"‚úÖ Total features: {X_train_raw.shape[1]}\")\n",
        "\n",
        "# Target\n",
        "y = train['price'].values.astype(np.float32)\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# Cleanup\n",
        "del train, test, train_basic_all, test_basic_all\n",
        "del bert_train_reduced, bert_test_reduced\n",
        "del tfidf_train_reduced, tfidf_test_reduced\n",
        "del train_image, test_image\n",
        "gc.collect()\n",
        "\n",
        "# ============================================================================\n",
        "# 7Ô∏è‚É£ TRAIN/VAL SPLIT\n",
        "# ============================================================================\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train_raw, y_log, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nüìö Train: {X_tr.shape[0]}, Val: {X_val.shape[0]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8Ô∏è‚É£ CRITICAL: SEPARATE SCALING FOR DIFFERENT MODELS\n",
        "# ============================================================================\n",
        "print(\"\\n‚öñÔ∏è Scaling features...\")\n",
        "\n",
        "# For tree models (XGB, LGB) - NO SCALING NEEDED\n",
        "X_tr_tree = X_tr.copy()\n",
        "X_val_tree = X_val.copy()\n",
        "X_test_tree = X_test_raw.copy()\n",
        "\n",
        "# For linear models (Ridge, ElasticNet) - MUST SCALE\n",
        "scaler_linear = RobustScaler()  # RobustScaler is safer than StandardScaler\n",
        "X_tr_linear = scaler_linear.fit_transform(X_tr).astype(np.float32)\n",
        "X_val_linear = scaler_linear.transform(X_val).astype(np.float32)\n",
        "X_test_linear = scaler_linear.transform(X_test_raw).astype(np.float32)\n",
        "\n",
        "print(\"‚úÖ Scaled separately for tree and linear models\")\n",
        "\n",
        "# ============================================================================\n",
        "# 8.5Ô∏è‚É£ SMART FEATURE SELECTION (BOOST ACCURACY)\n",
        "# ============================================================================\n",
        "print(\"\\nüéØ Feature selection for better performance...\")\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, VarianceThreshold\n",
        "\n",
        "# Step 1: Remove zero-variance features\n",
        "var_threshold = VarianceThreshold(threshold=0.01)\n",
        "X_tr_var = var_threshold.fit_transform(X_tr)\n",
        "X_val_var = var_threshold.transform(X_val)\n",
        "X_test_var = var_threshold.transform(X_test_raw)\n",
        "\n",
        "print(f\"   After variance filter: {X_tr.shape[1]} ‚Üí {X_tr_var.shape[1]} features\")\n",
        "\n",
        "# Step 2: Select top K important features\n",
        "n_features_to_keep = min(80, X_tr_var.shape[1])\n",
        "selector = SelectKBest(f_regression, k=n_features_to_keep)\n",
        "X_tr_selected = selector.fit_transform(X_tr_var, y_tr)\n",
        "X_val_selected = selector.transform(X_val_var)\n",
        "X_test_selected = selector.transform(X_test_var)\n",
        "\n",
        "print(f\"   Selected top {n_features_to_keep} features for modeling\")\n",
        "\n",
        "# Update feature matrices\n",
        "X_tr = X_tr_selected.copy()\n",
        "X_val = X_val_selected.copy()\n",
        "X_test_raw = X_test_selected.copy()\n",
        "\n",
        "del X_tr_var, X_val_var, X_test_var, X_tr_selected, X_val_selected, X_test_selected\n",
        "gc.collect()\n",
        "\n",
        "print(\"‚úÖ Feature selection complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# 9Ô∏è‚É£ ENHANCED MODEL TRAINING (TUNED HYPERPARAMS)\n",
        "# ============================================================================\n",
        "print(\"\\nüöÄ Training enhanced models...\")\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Re-create scaled versions after feature selection\n",
        "X_tr_tree = X_tr.copy()\n",
        "X_val_tree = X_val.copy()\n",
        "X_test_tree = X_test_raw.copy()\n",
        "\n",
        "scaler_linear = RobustScaler()\n",
        "X_tr_linear = scaler_linear.fit_transform(X_tr).astype(np.float32)\n",
        "X_val_linear = scaler_linear.transform(X_val).astype(np.float32)\n",
        "X_test_linear = scaler_linear.transform(X_test_raw).astype(np.float32)\n",
        "\n",
        "# Model 1: XGBoost (ENHANCED PARAMS + EARLY STOPPING)\n",
        "print(\"üå≥ Training XGBoost with early stopping...\")\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=7,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.2,\n",
        "    min_child_weight=3,\n",
        "    gamma=0.05,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist',\n",
        "    early_stopping_rounds=50  # XGBoost 2.0+ uses this directly\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Try new XGBoost 2.0+ API first\n",
        "    xgb_model.fit(\n",
        "        X_tr_tree, y_tr,\n",
        "        eval_set=[(X_val_tree, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "except TypeError:\n",
        "    # Fallback for older XGBoost versions\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=800,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=7,\n",
        "        subsample=0.85,\n",
        "        colsample_bytree=0.85,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.2,\n",
        "        min_child_weight=3,\n",
        "        gamma=0.05,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist'\n",
        "    )\n",
        "    xgb_model.fit(\n",
        "        X_tr_tree, y_tr,\n",
        "        eval_set=[(X_val_tree, y_val)],\n",
        "        callbacks=[xgb.callback.EarlyStopping(rounds=50, save_best=True)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "y_pred_xgb_val_log = xgb_model.predict(X_val_tree)\n",
        "y_val_orig = np.expm1(y_val)\n",
        "y_pred_xgb_val = np.expm1(y_pred_xgb_val_log)\n",
        "xgb_rmse = rmse(y_val_orig, y_pred_xgb_val)\n",
        "\n",
        "# Check if model has best_iteration attribute\n",
        "if hasattr(xgb_model, 'best_iteration') and xgb_model.best_iteration > 0:\n",
        "    print(f\"   ‚úÖ XGBoost RMSE: {xgb_rmse:.4f} (stopped at {xgb_model.best_iteration} trees)\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ XGBoost RMSE: {xgb_rmse:.4f}\")\n",
        "\n",
        "# Model 2: LightGBM (ENHANCED PARAMS + EARLY STOPPING)\n",
        "print(\"üí° Training LightGBM with early stopping...\")\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    num_leaves=63,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=0.15,\n",
        "    reg_lambda=0.15,\n",
        "    min_child_samples=20,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Try with callbacks (LightGBM 4.0+)\n",
        "    lgb_model.fit(\n",
        "        X_tr_tree, y_tr,\n",
        "        eval_set=[(X_val_tree, y_val)],\n",
        "        callbacks=[lgb.early_stopping(50, verbose=False)]\n",
        "    )\n",
        "except Exception:\n",
        "    # Fallback for older versions\n",
        "    lgb_model.fit(\n",
        "        X_tr_tree, y_tr,\n",
        "        eval_set=[(X_val_tree, y_val)],\n",
        "        eval_metric='rmse'\n",
        "    )\n",
        "\n",
        "y_pred_lgb_val_log = lgb_model.predict(X_val_tree)\n",
        "y_pred_lgb_val = np.expm1(y_pred_lgb_val_log)\n",
        "lgb_rmse = rmse(y_val_orig, y_pred_lgb_val)\n",
        "\n",
        "# Check if model has best_iteration attribute\n",
        "if hasattr(lgb_model, 'best_iteration_') and lgb_model.best_iteration_ > 0:\n",
        "    print(f\"   ‚úÖ LightGBM RMSE: {lgb_rmse:.4f} (stopped at {lgb_model.best_iteration_} trees)\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ LightGBM RMSE: {lgb_rmse:.4f}\")\n",
        "\n",
        "# Model 3: Ridge (TUNED ALPHA)\n",
        "print(\"üìê Training Ridge with optimized alpha...\")\n",
        "best_ridge_rmse = float('inf')\n",
        "best_ridge_model = None\n",
        "best_alpha = 10.0\n",
        "\n",
        "for alpha in [1.0, 5.0, 10.0, 20.0, 50.0]:\n",
        "    ridge_temp = Ridge(alpha=alpha, random_state=42)\n",
        "    ridge_temp.fit(X_tr_linear, y_tr)\n",
        "    y_pred_temp_log = ridge_temp.predict(X_val_linear)\n",
        "    y_pred_temp = np.clip(np.expm1(y_pred_temp_log), 0, 1e6)\n",
        "    temp_rmse = rmse(y_val_orig, y_pred_temp)\n",
        "\n",
        "    if temp_rmse < best_ridge_rmse:\n",
        "        best_ridge_rmse = temp_rmse\n",
        "        best_ridge_model = ridge_temp\n",
        "        best_alpha = alpha\n",
        "\n",
        "ridge_model = best_ridge_model\n",
        "y_pred_ridge_val_log = ridge_model.predict(X_val_linear)\n",
        "y_pred_ridge_val = np.clip(np.expm1(y_pred_ridge_val_log), 0, 1e6)\n",
        "ridge_rmse = best_ridge_rmse\n",
        "print(f\"   ‚úÖ Ridge RMSE: {ridge_rmse:.4f} (alpha={best_alpha})\")\n",
        "\n",
        "# Model 4: ElasticNet (TUNED PARAMS + INCREASED MAX_ITER)\n",
        "print(\"üéØ Training ElasticNet with optimized params...\")\n",
        "best_elastic_rmse = float('inf')\n",
        "best_elastic_model = None\n",
        "best_params = (0.01, 0.5)\n",
        "\n",
        "for alpha in [0.001, 0.01, 0.1]:\n",
        "    for l1_ratio in [0.2, 0.5, 0.8]:\n",
        "        elastic_temp = ElasticNet(\n",
        "            alpha=alpha,\n",
        "            l1_ratio=l1_ratio,\n",
        "            random_state=42,\n",
        "            max_iter=3000,  # Increased from 1500 to prevent convergence warnings\n",
        "            tol=1e-4\n",
        "        )\n",
        "        elastic_temp.fit(X_tr_linear, y_tr)\n",
        "        y_pred_temp_log = elastic_temp.predict(X_val_linear)\n",
        "        y_pred_temp = np.clip(np.expm1(y_pred_temp_log), 0, 1e6)\n",
        "        temp_rmse = rmse(y_val_orig, y_pred_temp)\n",
        "\n",
        "        if temp_rmse < best_elastic_rmse:\n",
        "            best_elastic_rmse = temp_rmse\n",
        "            best_elastic_model = elastic_temp\n",
        "            best_params = (alpha, l1_ratio)\n",
        "\n",
        "elastic_model = best_elastic_model\n",
        "y_pred_elastic_val_log = elastic_model.predict(X_val_linear)\n",
        "y_pred_elastic_val = np.clip(np.expm1(y_pred_elastic_val_log), 0, 1e6)\n",
        "elastic_rmse = best_elastic_rmse\n",
        "\n",
        "# Check for convergence warnings\n",
        "if not elastic_model.n_iter_ >= elastic_model.max_iter:\n",
        "    print(f\"   ‚úÖ ElasticNet RMSE: {elastic_rmse:.4f} (alpha={best_params[0]}, l1={best_params[1]})\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  ElasticNet RMSE: {elastic_rmse:.4f} (alpha={best_params[0]}, l1={best_params[1]}, converged={elastic_model.n_iter_}/{elastic_model.max_iter})\")\n",
        "\n",
        "# ============================================================================\n",
        "# üîü ENHANCED ENSEMBLE (LOG-SPACE BLENDING + REGULARIZATION)\n",
        "# ============================================================================\n",
        "print(\"\\nü§ù Building enhanced ensemble with log-space blending...\")\n",
        "\n",
        "# Store log-space predictions for safer blending\n",
        "model_predictions_log = {\n",
        "    'XGB': y_pred_xgb_val_log,\n",
        "    'LGB': y_pred_lgb_val_log,\n",
        "    'Ridge': y_pred_ridge_val_log,\n",
        "    'Elastic': y_pred_elastic_val_log\n",
        "}\n",
        "\n",
        "model_predictions = {\n",
        "    'XGB': y_pred_xgb_val,\n",
        "    'LGB': y_pred_lgb_val,\n",
        "    'Ridge': y_pred_ridge_val,\n",
        "    'Elastic': y_pred_elastic_val\n",
        "}\n",
        "\n",
        "model_rmse = {\n",
        "    'XGB': xgb_rmse,\n",
        "    'LGB': lgb_rmse,\n",
        "    'Ridge': ridge_rmse,\n",
        "    'Elastic': elastic_rmse\n",
        "}\n",
        "\n",
        "# Filter valid models\n",
        "valid_models = [(name, pred_log, pred, rmse_val)\n",
        "                for name, pred_log, pred, rmse_val in\n",
        "                [(k, model_predictions_log[k], model_predictions[k], model_rmse[k])\n",
        "                 for k in model_predictions.keys()]\n",
        "                if rmse_val < 1e6]\n",
        "\n",
        "print(f\"   Valid models: {len(valid_models)}\")\n",
        "\n",
        "# Calculate regularized weights (inverse RMSE with epsilon)\n",
        "eps = 1e-6\n",
        "total_inv_rmse = sum(1 / (m[3] + eps) for m in valid_models)\n",
        "weights = {m[0]: (1 / (m[3] + eps)) / total_inv_rmse for m in valid_models}\n",
        "\n",
        "print(f\"   Regularized weights: {weights}\")\n",
        "\n",
        "# CRITICAL: Blend in log space for numerical stability\n",
        "y_ensemble_val_log = sum(weights[m[0]] * m[1] for m in valid_models)\n",
        "y_ensemble_val = np.expm1(y_ensemble_val_log)\n",
        "y_ensemble_val = np.clip(y_ensemble_val, 0, 1e6)  # Safety\n",
        "\n",
        "ensemble_rmse = rmse(y_val_orig, y_ensemble_val)\n",
        "ensemble_mae = mean_absolute_error(y_val_orig, y_ensemble_val)\n",
        "\n",
        "print(f\"\\nüìä ENHANCED VALIDATION RESULTS:\")\n",
        "print(f\"   XGBoost:    {xgb_rmse:.4f}\")\n",
        "print(f\"   LightGBM:   {lgb_rmse:.4f}\")\n",
        "print(f\"   Ridge:      {ridge_rmse:.4f}\")\n",
        "print(f\"   ElasticNet: {elastic_rmse:.4f}\")\n",
        "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"   ENSEMBLE:   {ensemble_rmse:.4f} ‚≠ê\")\n",
        "print(f\"   MAE:        {ensemble_mae:.4f}\")\n",
        "\n",
        "# Calculate improvement\n",
        "best_individual = min(model_rmse.values())\n",
        "improvement = ((best_individual - ensemble_rmse) / best_individual) * 100\n",
        "print(f\"   Improvement: {improvement:.2f}% better than best model\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ ENHANCED FINAL PREDICTIONS (LOG-SPACE BLENDING)\n",
        "# ============================================================================\n",
        "print(\"\\nüéØ Generating enhanced final predictions...\")\n",
        "\n",
        "# Predict in chunks to avoid RAM issues\n",
        "def safe_predict_log(model, X, chunk_size=10000):\n",
        "    \"\"\"Predict in log space with chunking\"\"\"\n",
        "    preds = []\n",
        "    for i in range(0, len(X), chunk_size):\n",
        "        chunk = X[i:i+chunk_size]\n",
        "        pred = model.predict(chunk)\n",
        "        preds.extend(pred)\n",
        "        if i % 50000 == 0 and i > 0:\n",
        "            print(f\"      {i}/{len(X)} done\")\n",
        "            gc.collect()\n",
        "    return np.array(preds)\n",
        "\n",
        "# Generate test predictions in LOG SPACE\n",
        "print(\"   XGBoost predictions (log-space)...\")\n",
        "xgb_test_log = safe_predict_log(xgb_model, X_test_tree)\n",
        "\n",
        "print(\"   LightGBM predictions (log-space)...\")\n",
        "lgb_test_log = safe_predict_log(lgb_model, X_test_tree)\n",
        "\n",
        "print(\"   Ridge predictions (log-space)...\")\n",
        "ridge_test_log = safe_predict_log(ridge_model, X_test_linear)\n",
        "\n",
        "print(\"   ElasticNet predictions (log-space)...\")\n",
        "elastic_test_log = safe_predict_log(elastic_model, X_test_linear)\n",
        "\n",
        "# CRITICAL: Blend in log space\n",
        "print(\"\\n   Blending in log-space for numerical stability...\")\n",
        "final_preds_log = (\n",
        "    weights.get('XGB', 0) * xgb_test_log +\n",
        "    weights.get('LGB', 0) * lgb_test_log +\n",
        "    weights.get('Ridge', 0) * ridge_test_log +\n",
        "    weights.get('Elastic', 0) * elastic_test_log\n",
        ")\n",
        "\n",
        "# Convert back to original scale\n",
        "final_preds = np.expm1(final_preds_log)\n",
        "\n",
        "# Safety checks\n",
        "final_preds = np.clip(final_preds, 0.01, 1e6)  # Reasonable price range\n",
        "final_preds = np.nan_to_num(final_preds, nan=10.0)  # Replace any NaNs\n",
        "\n",
        "print(f\"‚úÖ Log-space blending complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ CREATE SUBMISSION\n",
        "# ============================================================================\n",
        "print(\"\\nüíæ Creating submission...\")\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_id': test_ids,\n",
        "    'price': final_preds\n",
        "})\n",
        "\n",
        "print(f\"\\n‚úÖ SUBMISSION STATS:\")\n",
        "print(f\"   Samples: {len(submission):,}\")\n",
        "print(f\"   Price range: ${submission['price'].min():.2f} - ${submission['price'].max():.2f}\")\n",
        "print(f\"   Mean: ${submission['price'].mean():.2f}\")\n",
        "print(f\"   Median: ${submission['price'].median():.2f}\")\n",
        "print(f\"   Valid prices: {(submission['price'] > 0).sum()}/{len(submission)}\")\n",
        "\n",
        "# Save\n",
        "submission.to_csv('enhanced_bert_ensemble_v2.csv', index=False)\n",
        "print(f\"\\nüéâ SAVED: enhanced_bert_ensemble_v2.csv\")\n",
        "\n",
        "# Cleanup\n",
        "del X_train_raw, X_test_raw, X_tr, X_val, y_tr, y_val\n",
        "del X_tr_tree, X_val_tree, X_test_tree\n",
        "del X_tr_linear, X_val_linear, X_test_linear\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ ENHANCED PIPELINE COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üéØ Key Improvements Applied:\")\n",
        "print(f\"   ‚úì Feature selection (top {n_features_to_keep} features)\")\n",
        "print(f\"   ‚úì Early stopping for tree models\")\n",
        "print(f\"   ‚úì Hyperparameter tuning (Ridge alpha, ElasticNet)\")\n",
        "print(f\"   ‚úì Log-space blending (safer ensemble)\")\n",
        "print(f\"   ‚úì Regularized ensemble weights (eps={eps})\")\n",
        "print(f\"   ‚úì {improvement:.2f}% improvement over best individual model\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "RuBoP-Behs0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}