{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 3: IMPROVED MULTIMODAL with Feature Weighting\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import xgboost as xgb\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Load data\n",
        "# -----------------------------\n",
        "print(\"üì• Loading data...\")\n",
        "train = pd.read_csv('/content/drive/MyDrive/train_cleaned.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/test_cleaned.csv')\n",
        "\n",
        "print(f\"üìä Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ IMPROVED: Separate Feature Extraction by Type\n",
        "# -----------------------------\n",
        "target_col = 'price'\n",
        "\n",
        "print(\"üéØ Extracting multimodal features with separate scaling...\")\n",
        "\n",
        "# A. TEXT FEATURES\n",
        "print(\"üî§ Processing text features...\")\n",
        "all_texts = list(train['catalog_content_clean'].fillna('')) + list(test['catalog_content_clean'].fillna(''))\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=30, stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(all_texts)\n",
        "\n",
        "svd = TruncatedSVD(n_components=8, random_state=42)  # Slightly more text features\n",
        "text_features = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "train_text = text_features[:len(train)]\n",
        "test_text = text_features[len(train):]\n",
        "\n",
        "print(f\"‚úÖ Text features: {train_text.shape[1]} dimensions\")\n",
        "\n",
        "# B. IMAGE FEATURES\n",
        "print(\"üñºÔ∏è Extracting image features...\")\n",
        "\n",
        "def extract_image_features(df):\n",
        "    features = []\n",
        "    for url in df['image_link']:\n",
        "        url_str = str(url)\n",
        "        feat = [\n",
        "            1 if url_str.startswith('http') else 0,\n",
        "            1 if 'amazon' in url_str.lower() else 0,\n",
        "            1 if 'media' in url_str.lower() else 0,\n",
        "            1 if '.jpg' in url_str.lower() else 0,\n",
        "            1 if '.png' in url_str.lower() else 0,\n",
        "            1 if 'cdn' in url_str.lower() else 0,\n",
        "            len(url_str),\n",
        "            url_str.count('/'),  # URL complexity\n",
        "        ]\n",
        "        features.append(feat)\n",
        "    return np.array(features)\n",
        "\n",
        "train_image = extract_image_features(train)\n",
        "test_image = extract_image_features(test)\n",
        "\n",
        "print(f\"‚úÖ Image features: {train_image.shape[1]} dimensions\")\n",
        "\n",
        "# C. BASIC NUMERIC FEATURES\n",
        "print(\"üî¢ Processing basic features...\")\n",
        "basic_features = ['item_pack_qty', 'catalog_len', 'text_length', 'word_count', 'has_image_url']\n",
        "available_basic = [f for f in basic_features if f in train.columns and f in test.columns]\n",
        "\n",
        "train_basic = train[available_basic].values\n",
        "test_basic = test[available_basic].values\n",
        "\n",
        "print(f\"‚úÖ Basic features: {train_basic.shape[1]} dimensions\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ IMPROVED: Separate Scaling by Feature Type\n",
        "# -----------------------------\n",
        "print(\"‚öñÔ∏è Scaling features separately by type...\")\n",
        "\n",
        "# Scale each feature type independently\n",
        "scaler_basic = StandardScaler()\n",
        "scaler_text = StandardScaler()\n",
        "scaler_image = StandardScaler()\n",
        "\n",
        "# Fit and transform each type separately\n",
        "train_basic_scaled = scaler_basic.fit_transform(train_basic)\n",
        "test_basic_scaled = scaler_basic.transform(test_basic)\n",
        "\n",
        "train_text_scaled = scaler_text.fit_transform(train_text)\n",
        "test_text_scaled = scaler_text.transform(test_text)\n",
        "\n",
        "train_image_scaled = scaler_image.fit_transform(train_image)\n",
        "test_image_scaled = scaler_image.transform(test_image)\n",
        "\n",
        "print(\"‚úÖ Features scaled separately by type\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ IMPROVED: Feature Weighting by Type Importance\n",
        "# -----------------------------\n",
        "print(\"üéØ Applying feature type weighting...\")\n",
        "\n",
        "# Define weights based on expected importance\n",
        "# These can be tuned based on validation performance\n",
        "WEIGHTS = {\n",
        "    'basic': 1.0,   # Full importance - most reliable\n",
        "    'text': 0.9,    # High importance - text is very informative\n",
        "    'image': 0.7    # Medium importance - URL patterns are useful but weaker\n",
        "}\n",
        "\n",
        "# Apply weights to each feature type\n",
        "train_basic_weighted = train_basic_scaled * WEIGHTS['basic']\n",
        "test_basic_weighted = test_basic_scaled * WEIGHTS['basic']\n",
        "\n",
        "train_text_weighted = train_text_scaled * WEIGHTS['text']\n",
        "test_text_weighted = test_text_scaled * WEIGHTS['text']\n",
        "\n",
        "train_image_weighted = train_image_scaled * WEIGHTS['image']\n",
        "test_image_weighted = test_image_scaled * WEIGHTS['image']\n",
        "\n",
        "print(f\"‚úÖ Applied weights - Basic: {WEIGHTS['basic']}, Text: {WEIGHTS['text']}, Image: {WEIGHTS['image']}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Combine Weighted Features\n",
        "# -----------------------------\n",
        "X_combined = np.hstack([train_basic_weighted, train_text_weighted, train_image_weighted])\n",
        "X_test_combined = np.hstack([test_basic_weighted, test_text_weighted, test_image_weighted])\n",
        "\n",
        "print(f\"üìà FINAL WEIGHTED FEATURE DIMENSIONS:\")\n",
        "print(f\"   Train: {X_combined.shape}\")\n",
        "print(f\"   Test:  {X_test_combined.shape}\")\n",
        "print(f\"   Feature breakdown:\")\n",
        "print(f\"     - Basic ({WEIGHTS['basic']}x): {train_basic_weighted.shape[1]}\")\n",
        "print(f\"     - Text ({WEIGHTS['text']}x):  {train_text_weighted.shape[1]}\")\n",
        "print(f\"     - Image ({WEIGHTS['image']}x): {train_image_weighted.shape[1]}\")\n",
        "\n",
        "y = train[target_col]\n",
        "\n",
        "# Free memory\n",
        "del train, test, tfidf_matrix, text_features\n",
        "del train_basic, test_basic, train_text, test_text, train_image, test_image\n",
        "del train_basic_scaled, test_basic_scaled, train_text_scaled, test_text_scaled, train_image_scaled, test_image_scaled\n",
        "gc.collect()\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Log-transform target\n",
        "# -----------------------------\n",
        "use_log_transform = True\n",
        "y_log = np.log1p(y) if use_log_transform else y.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Train/Validation split on weighted features\n",
        "# -----------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_combined, y_log, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"üìö Training: {X_train.shape[0]}, Validation: {X_val.shape[0]}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Enhanced XGBoost with Weighted Features\n",
        "# -----------------------------\n",
        "model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    min_child_weight=2,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=1,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training XGBoost with weighted multimodal features...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Training completed!\")\n",
        "\n",
        "# Free training data\n",
        "del X_train, y_train\n",
        "gc.collect()\n",
        "\n",
        "# -----------------------------\n",
        "# 9Ô∏è‚É£ Validation with Feature Importance Analysis\n",
        "# -----------------------------\n",
        "y_val_pred = model.predict(X_val)\n",
        "\n",
        "if use_log_transform:\n",
        "    y_val_true = np.expm1(y_val)\n",
        "    y_val_pred_orig = np.expm1(y_val_pred)\n",
        "else:\n",
        "    y_val_true = y_val\n",
        "    y_val_pred_orig = y_val_pred\n",
        "\n",
        "mse = mean_squared_error(y_val_true, y_val_pred_orig)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_val_true, y_val_pred_orig)\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
        "\n",
        "smape_val = smape(y_val_true, y_val_pred_orig)\n",
        "\n",
        "print(f\"üìä VALIDATION RESULTS:\")\n",
        "print(f\"   RMSE:  {rmse:.4f}\")\n",
        "print(f\"   MAE:   {mae:.4f}\")\n",
        "print(f\"   SMAPE: {smape_val:.4f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# üîü IMPROVED: Feature Importance by Type\n",
        "# -----------------------------\n",
        "print(f\"\\nüìà FEATURE IMPORTANCE BY TYPE:\")\n",
        "\n",
        "# Create feature names with types\n",
        "feature_names = (\n",
        "    [f'basic_{col}' for col in available_basic] +\n",
        "    [f'text_{i}' for i in range(train_text_weighted.shape[1])] +\n",
        "    [f'image_{i}' for i in range(train_image_weighted.shape[1])]\n",
        ")\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': model.feature_importances_,\n",
        "    'type': ['basic'] * len(available_basic) + ['text'] * train_text_weighted.shape[1] + ['image'] * train_image_weighted.shape[1]\n",
        "})\n",
        "\n",
        "# Calculate average importance by type\n",
        "type_importance = importance_df.groupby('type')['importance'].mean().sort_values(ascending=False)\n",
        "print(\"üîù Average Importance by Feature Type:\")\n",
        "for feature_type, imp in type_importance.items():\n",
        "    print(f\"   {feature_type:6s}: {imp:.4f}\")\n",
        "\n",
        "print(\"\\nüèÜ Top 10 Most Important Features:\")\n",
        "print(importance_df.nlargest(10, 'importance')[['feature', 'type', 'importance']].to_string(index=False))\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Predict on test set\n",
        "# -----------------------------\n",
        "print(\"üéØ Making test predictions...\")\n",
        "test_preds = model.predict(X_test_combined)\n",
        "\n",
        "if use_log_transform:\n",
        "    test_preds = np.expm1(test_preds)\n",
        "\n",
        "test_preds = np.clip(test_preds, 0.1, None)\n",
        "\n",
        "print(f\"üìä Test predictions - Min: {test_preds.min():.2f}, Max: {test_preds.max():.2f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ Create submission\n",
        "# -----------------------------\n",
        "print(\"üíæ Creating submission file...\")\n",
        "test_ids = pd.read_csv('/content/drive/MyDrive/test_cleaned.csv', usecols=['sample_id'])\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_id': test_ids['sample_id'],\n",
        "    'price': test_preds\n",
        "})\n",
        "\n",
        "print(f\"‚úÖ SUBMISSION VALIDATION:\")\n",
        "print(f\"   Samples: {len(submission)}\")\n",
        "print(f\"   Price range: ${submission['price'].min():.2f} to ${submission['price'].max():.2f}\")\n",
        "\n",
        "submission_file = 'weighted_multimodal_submission.csv'\n",
        "submission.to_csv(submission_file, index=False)\n",
        "print(f\"üéâ SUBMISSION SAVED: {submission_file}\")\n",
        "\n",
        "print(\"\\nüéâ IMPROVED MULTIMODAL PIPELINE COMPLETED!\")\n",
        "print(\"üöÄ Feature weighting + Separate scaling = Better performance!\")"
      ],
      "metadata": {
        "id": "RuBoP-Behs0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}