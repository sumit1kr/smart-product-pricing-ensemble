{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6OC047AhJY4"
      },
      "outputs": [],
      "source": [
        "# ---------- Helpers ----------\n",
        "def load_data():\n",
        "    import pandas as pd\n",
        "    train = pd.read_csv(TRAIN_CSV)\n",
        "    test  = pd.read_csv(TEST_CSV)\n",
        "    return train, test\n",
        "\n",
        "def quick_inspect(df, name=\"dataframe\", n=5):\n",
        "    print(f\"--- {name} shape: {df.shape} ---\")\n",
        "    display(df.head(n))\n",
        "    print(df.info())\n",
        "    print(\"Null counts:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"sample_id duplicates:\", df['sample_id'].duplicated().sum())\n",
        "    if 'price' in df.columns:\n",
        "        print(\"price summary:\")\n",
        "        print(df['price'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Text cleaning ----------\n",
        "def clean_text(s):\n",
        "    import pandas as pd\n",
        "    import re\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = str(s)\n",
        "    s = s.replace(\"\\n\", \" \").replace(\"\\r\",\" \")\n",
        "    # remove HTML tags\n",
        "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
        "    # remove URLs\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
        "    # normalize unicode weirdness\n",
        "    s = re.sub(r\"[\\u2000-\\u206F\\u2E00-\\u2E7F\\\\'\\\"()]\", \" \", s)\n",
        "    # keep letters, numbers, common punctuation .,%- and spaces\n",
        "    s = re.sub(r\"[^A-Za-z0-9\\.\\,\\%\\-\\s/]\", \" \", s)\n",
        "    # unify whitespace\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s.lower()"
      ],
      "metadata": {
        "id": "64qbWc5chQwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Image download with retry ----------\n",
        "def download_image(url, dest_path, timeout=8, max_retries=3):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible)\"}\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = requests.get(url, stream=True, timeout=timeout, headers=headers)\n",
        "            resp.raise_for_status()\n",
        "            # verify small image open\n",
        "            img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
        "            img.save(dest_path, format=\"JPEG\", quality=85)\n",
        "            return {\"ok\": True, \"size\": dest_path.stat().st_size}\n",
        "        except Exception as e:\n",
        "            time.sleep(1 + attempt*1.5)\n",
        "            last_err = str(e)\n",
        "    return {\"ok\": False, \"error\": last_err}"
      ],
      "metadata": {
        "id": "OfZqPMfZhTxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Main cleaning function ----------\n",
        "def clean_dataset(train, test, download_images=False, max_images=None):\n",
        "    # 1. basic inspection\n",
        "    quick_inspect(train, \"train (raw)\")\n",
        "    quick_inspect(test, \"test (raw)\")\n",
        "\n",
        "    # 2. sample_id uniqueness\n",
        "    if train['sample_id'].duplicated().any():\n",
        "        print(\"Warning: duplicated sample_id in train - keeping first occurrence.\")\n",
        "        train = train.drop_duplicates(subset=['sample_id'], keep='first')\n",
        "    if test['sample_id'].duplicated().any():\n",
        "        test = test.drop_duplicates(subset=['sample_id'], keep='first')\n",
        "\n",
        "    # 3. clean text\n",
        "    print(\"Cleaning catalog_content ...\")\n",
        "    train['catalog_content_clean'] = train['catalog_content'].apply(clean_text)\n",
        "    test['catalog_content_clean']  = test['catalog_content'].apply(clean_text)\n",
        "\n",
        "    # 4. extract item pack qty\n",
        "    print(\"Extracting item_pack_qty ...\")\n",
        "    train['item_pack_qty'] = train['catalog_content_clean'].apply(extract_ipq)\n",
        "    test['item_pack_qty']  = test['catalog_content_clean'].apply(extract_ipq)\n",
        "\n",
        "    # 5. handle missing images\n",
        "    for df,name in [(train,\"train\"), (test,\"test\")]:\n",
        "        df['image_link'] = df['image_link'].fillna(\"\")\n",
        "        df['has_image_url'] = df['image_link'].apply(lambda x: bool(str(x).strip()))\n",
        "\n",
        "    # 6. handle missing price in train (drop and log)\n",
        "    if train['price'].isnull().any():\n",
        "        nnull = train['price'].isnull().sum()\n",
        "        print(f\"Found {nnull} null prices in train â€” dropping those rows.\")\n",
        "        train = train[~train['price'].isnull()].copy()\n",
        "\n",
        "    # 7. outlier clipping + log transform option\n",
        "    print(\"Clipping extreme prices and creating price_log ...\")\n",
        "    train, low, high = clip_prices(train)\n",
        "    train['price_log'] = np.log1p(train['price_clipped'])\n",
        "\n",
        "    # 8. short catalog / empty handling - keep flag\n",
        "    for df,name in [(train,\"train\"), (test,\"test\")]:\n",
        "        df['catalog_len'] = df['catalog_content_clean'].apply(lambda x: len(str(x).split()))\n",
        "        df['catalog_short'] = df['catalog_len'] < 3  # flag very short descriptions\n",
        "\n",
        "    # 9. image manifest + optional download (do in batches)\n",
        "    manifest_rows = []\n",
        "    def manifest_row(row, idx):\n",
        "        url = row['image_link']\n",
        "        sid = row['sample_id']\n",
        "        dest = IMG_DIR / f\"{sid}.jpg\"\n",
        "        if url.strip()==\"\":\n",
        "            return {\"sample_id\": sid, \"image_link\": url, \"has_image\": False, \"path\": \"\", \"ok\": False, \"error\": \"no-url\"}\n",
        "        if download_images:\n",
        "            r = download_image(url, dest)\n",
        "            return {\"sample_id\": sid, \"image_link\": url, \"has_image\": r['ok'], \"path\": str(dest) if r['ok'] else \"\", \"ok\": r['ok'], \"error\": r.get('error','')}\n",
        "        else:\n",
        "            return {\"sample_id\": sid, \"image_link\": url, \"has_image\": True, \"path\": str(dest), \"ok\": None, \"error\": \"\"}\n",
        "\n",
        "    print(\"Creating image manifest (no download unless download_images=True)...\")\n",
        "    # Merge train + test sample ids but keep order: train then test if desired\n",
        "    for df in [train.head(max_images) if max_images else train, test.head(max_images) if max_images else test]:\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            manifest_rows.append(manifest_row(row, idx))\n",
        "    manifest = pd.DataFrame(manifest_rows)\n",
        "    manifest.to_csv(IMAGE_MANIFEST, index=False)\n",
        "\n",
        "    # 10. save cleaned datasets\n",
        "    train.to_csv(TRAIN_OUT, index=False)\n",
        "    test.to_csv(TEST_OUT, index=False)\n",
        "    print(\"Saved cleaned files:\", TRAIN_OUT, TEST_OUT)\n",
        "    print(\"Saved image manifest:\", IMAGE_MANIFEST)\n",
        "    return train, test, manifest\n"
      ],
      "metadata": {
        "id": "hXcz34uShV3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_prices(df, lower_quantile=0.001, upper_quantile=0.999):\n",
        "    \"\"\"\n",
        "    Clips extreme prices based on quantiles and adds a clipped price column.\n",
        "    Logs the lower and upper bounds used for clipping.\n",
        "    \"\"\"\n",
        "    if 'price' not in df.columns:\n",
        "        print(\"Warning: 'price' column not found for clipping.\")\n",
        "        return df, None, None\n",
        "\n",
        "    low = df['price'].quantile(lower_quantile)\n",
        "    high = df['price'].quantile(upper_quantile)\n",
        "    print(f\"Clipping prices: lower bound = {low:.2f}, upper bound = {high:.2f}\")\n",
        "    df['price_clipped'] = df['price'].clip(lower=low, upper=high)\n",
        "    return df, low, high"
      ],
      "metadata": {
        "id": "HQJTygSmhXtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Main cleaning function ----------\n",
        "def clean_dataset(train, test, download_images=False, max_images=None):\n",
        "    import numpy as np # Import numpy here\n",
        "    # 1. basic inspection\n",
        "    quick_inspect(train, \"train (raw)\")\n",
        "    quick_inspect(test, \"test (raw)\")\n",
        "\n",
        "    # 2. sample_id uniqueness\n",
        "    if train['sample_id'].duplicated().any():\n",
        "        print(\"Warning: duplicated sample_id in train - keeping first occurrence.\")\n",
        "        train = train.drop_duplicates(subset=['sample_id'], keep='first').copy()\n",
        "    if test['sample_id'].duplicated().any():\n",
        "        test = test.drop_duplicates(subset=['sample_id'], keep='first').copy()\n",
        "\n",
        "    # 3. clean text\n",
        "    print(\"Cleaning catalog_content ...\")\n",
        "    train['catalog_content_clean'] = train['catalog_content'].apply(clean_text)\n",
        "    test['catalog_content_clean']  = test['catalog_content'].apply(clean_text)\n",
        "\n",
        "    # 4. extract item pack qty\n",
        "    print(\"Extracting item_pack_qty ...\")\n",
        "    train['item_pack_qty'] = train['catalog_content_clean'].apply(extract_ipq)\n",
        "    test['item_pack_qty']  = test['catalog_content_clean'].apply(extract_ipq)\n",
        "\n",
        "    # 5. handle missing images\n",
        "    for df,name in [(train,\"train\"), (test,\"test\")]:\n",
        "        df['image_link'] = df['image_link'].fillna(\"\")\n",
        "        df['has_image_url'] = df['image_link'].apply(lambda x: bool(str(x).strip()))\n",
        "\n",
        "    # 6. handle missing price in train (drop and log)\n",
        "    if train['price'].isnull().any():\n",
        "        nnull = train['price'].isnull().sum()\n",
        "        print(f\"Found {nnull} null prices in train â€” dropping those rows.\")\n",
        "        train = train[~train['price'].isnull()].copy()\n",
        "\n",
        "    # 7. outlier clipping + log transform option\n",
        "    print(\"Clipping extreme prices and creating price_log ...\")\n",
        "    train, low, high = clip_prices(train)\n",
        "    train['price_log'] = np.log1p(train['price_clipped'])\n",
        "\n",
        "    # 8. short catalog / empty handling - keep flag\n",
        "    for df,name in [(train,\"train\"), (test,\"test\")]:\n",
        "        df['catalog_len'] = df['catalog_content_clean'].apply(lambda x: len(str(x).split()))\n",
        "        df['catalog_short'] = df['catalog_len'] < 3  # flag very short descriptions\n",
        "\n",
        "    # 9. image manifest + optional download (do in batches)\n",
        "    manifest_rows = []\n",
        "    def manifest_row(row, idx):\n",
        "        url = row['image_link']\n",
        "        sid = row['sample_id']\n",
        "        dest = IMG_DIR / f\"{sid}.jpg\"\n",
        "        if url.strip()==\"\":\n",
        "            return {\"sample_id\": sid, \"image_link\": url, \"has_image\": False, \"path\": \"\", \"ok\": False, \"error\": \"no-url\"}\n",
        "        if download_images:\n",
        "            r = download_image(url, dest)\n",
        "            return {\"sample_id\": sid, \"image_link\": url, \"has_image\": r['ok'], \"path\": str(dest) if r['ok'] else \"\", \"ok\": r['ok'], \"error\": r.get('error','')}\n",
        "        else:\n",
        "            return {\"sample_id\": sid, \"image_link\": url, \"has_image\": True, \"path\": str(dest), \"ok\": None, \"error\": \"\"}\n",
        "\n",
        "    print(\"Creating image manifest (no download unless download_images=True)...\")\n",
        "    # Merge train + test sample ids but keep order: train then test if desired\n",
        "    for df in [train.head(max_images) if max_images else train, test.head(max_images) if max_images else test]:\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            manifest_rows.append(manifest_row(row, idx))\n",
        "    manifest = pd.DataFrame(manifest_rows)\n",
        "    manifest.to_csv(IMAGE_MANIFEST, index=False)\n",
        "\n",
        "    # 10. save cleaned datasets\n",
        "    train.to_csv(TRAIN_OUT, index=False)\n",
        "    test.to_csv(TEST_OUT, index=False)\n",
        "    print(\"Saved cleaned files:\", TRAIN_OUT, TEST_OUT)\n",
        "    print(\"Saved image manifest:\", IMAGE_MANIFEST)\n",
        "    return train, test, manifest"
      ],
      "metadata": {
        "id": "7MoIoJ25hZ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Run cleaning ----------\n",
        "if __name__ == \"__main__\":\n",
        "    train, test = load_data()\n",
        "    # For initial run we do NOT download images: set download_images=True in a later cell if desired\n",
        "    train_cleaned, test_cleaned, manifest = clean_dataset(train, test, download_images=False, max_images=None)\n",
        "    print(\"Done Step -1 cleaning.\")"
      ],
      "metadata": {
        "id": "uR_rHvuxhcVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}